{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ee669c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install langchain_community tiktoken langchain-openai langchainhub chromadb langchain langchain-cohere langchain-upstage langchain-google-genai langchain-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea98a0f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = '' # put your langcahin api key\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = 'default'\n",
    "os.environ['GOOGLE_API_KEY'] = '' # put your google api key\n",
    "os.environ['OPENAI_API_KEY'] = '' # put your open api key\n",
    "os.environ[\"UPSTAGE_API_KEY\"] = \"\"\n",
    "os.environ[\"PPLX_API_KEY\"] = ''\n",
    "os.environ[\"COHERE_API_KEY\"] = ''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a236c436",
   "metadata": {},
   "source": [
    "Part 1: Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b4db29",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703f8d34",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_huggingface import HuggingFacePipeline, HuggingFaceEmbeddings\n",
    "from langchain_cohere import ChatCohere\n",
    "from langchain_cohere import CohereEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_upstage import ChatUpstage\n",
    "from langchain_upstage import UpstageEmbeddings\n",
    "\n",
    "import os\n",
    "import getpass\n",
    "\n",
    "#### INDEXING ####\n",
    "\n",
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google AI API key: \")\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your Google AI API key: \")\n",
    "\n",
    "# LLM\n",
    "# Initialize the LLM with the desired model ID and API token\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "mode = \"upstage\" # openai, hf, cohere, google, cohere, upstage\n",
    "if mode == \"hf\":\n",
    "    model_id = \"skt/A.X-4.0-Light\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_id\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "    pipe = pipeline(\"text-generation\", model=model,\n",
    "                tokenizer=tokenizer, max_new_tokens=512)\n",
    "\n",
    "    llm = HuggingFacePipeline(pipeline=pipe)\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "\n",
    "elif mode == \"cohere\":\n",
    "    #https://dashboard.cohere.com/api-keys #put your cohere API key\n",
    "    llm = ChatCohere()\n",
    "    embeddings = CohereEmbeddings(model=\"embed-english-v3.0\",)\n",
    "\n",
    "\n",
    "elif mode == \"openai\":\n",
    "    #put your OpenAI api key\n",
    "    llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "\n",
    "elif mode == \"google\":\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-pro-preview-03-25\",\n",
    "                temperature=0,\n",
    "                max_tokens=None,\n",
    "                timeout=None,\n",
    "                max_retries=2,\n",
    "              )\n",
    "    embeddings = GoogleGenerativeAIEmbeddings(model='embedding-001',\n",
    "                                              output_dimensionality=1024)\n",
    "\n",
    "elif mode == \"upstage\":\n",
    "    llm = ChatUpstage()\n",
    "    embeddings = UpstageEmbeddings(model=\"embedding-query\", dimensions=1024)\n",
    "else:\n",
    "    raise Exception(\"no matched mode\")\n",
    "\n",
    "\n",
    "\n",
    "# Load Documents\n",
    "\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=[\n",
    "        \"https://n.news.naver.com/article/437/0000378416\",\n",
    "        \"https://n.news.naver.com/mnews/hotissue/article/092/0002340014?type=series&cid=2000063\",\n",
    "    ],\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            \"div\",\n",
    "            attrs={\"class\": [\"newsct_article _article_body\", \"media_end_head_title\"]},\n",
    "        )\n",
    "    ),\n",
    "    header_template={\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36\",\n",
    "    },\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Embed\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "from langsmith import Client\n",
    "client = Client(api_key='') #<===your langchain api key!!!!\n",
    "prompt = client.pull_prompt(\"rlm/rag-prompt\", include_model=True)\n",
    "\n",
    "#### RETRIEVAL and GENERATION ####\n",
    "\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# Chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Question\n",
    "rag_chain.invoke(\"저출생 정책?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f293766",
   "metadata": {},
   "source": [
    "Part 2: Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13ef613",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Documents\n",
    "question = \"What kinds of pets do I like?\"\n",
    "document = \"My favorite pet is a cat.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a6b444",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "num_tokens_from_string(question, \"cl100k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfc2803",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "if embeddings == Null:\n",
    "  embeddings = OpenAIEmbeddings()\n",
    "query_result = embedding.embed_query(question)\n",
    "document_result = embeddings.embed_query(document)\n",
    "len(query_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b91371",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "similarity = cosine_similarity(query_result, document_result)\n",
    "print(\"Cosine Similarity:\", similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87965ba5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#document loader -- reusing above loader\n",
    "\n",
    "# Split\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50)\n",
    "\n",
    "# Make splits\n",
    "splits = text_splitter.split_documents(blog_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91c59c2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Index\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "vectorstore = Chroma.from_documents(documents=splits,\n",
    "                                    embedding=embeddings)\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee329758",
   "metadata": {},
   "source": [
    "Part 3: Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0b9192",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Index\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "vectorstore = Chroma.from_documents(documents=splits,\n",
    "                                    embedding=embeddings)\n",
    "\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})\n",
    "docs = retriever.get_relevant_documents(\"저출생 정책?\")\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da4b1a3",
   "metadata": {},
   "source": [
    "Part 4: Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6875012b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Prompt\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833fa0b1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Chain\n",
    "chain = prompt | llm\n",
    "# Run\n",
    "chain.invoke({\"context\":docs,\"question\":저출생 정책?\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329df4a7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"저출생 정책?\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
