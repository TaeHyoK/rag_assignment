{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b0ef5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain_community tiktoken langchain-openai langchainhub chromadb langchain langchain-cohere langchain-upstage langchain-google-genai langchain-huggingface pypdf python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f97dd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'false'  # 트레이싱 비활성화\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv('langchain_api_key', '')\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = 'default'\n",
    "os.environ['GOOGLE_API_KEY'] = os.getenv('gemini_api_key', '')\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('openai_api_key', '')\n",
    "# os.environ[\"UPSTAGE_API_KEY\"] = os.getenv('upstage_api_key', '')\n",
    "# os.environ[\"PPLX_API_KEY\"] = os.getenv('pplx_api_key', '')\n",
    "# os.environ[\"COHERE_API_KEY\"] = os.getenv('cohere_api_key', '')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cceb5e7",
   "metadata": {},
   "source": [
    "Part 1: Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bcde1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605c4b4d",
   "metadata": {},
   "source": [
    "### pypdfloader 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b4e83871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 PDF files\n",
      "✓ Loaded pdf/SnT GPS_287호_최종.pdf: 70 pages\n",
      "✗ Error loading pdf/SnT GPS_290호_최종본.pdf: unhashable type: 'ArrayObject'\n",
      "✓ Loaded pdf/SnT GPS_286호_최종.pdf: 68 pages\n",
      "✗ Error loading pdf/SnT GPS_288호_최종.pdf: unhashable type: 'ArrayObject'\n",
      "✗ Error loading pdf/SnT GPS_285호_최종.pdf: unhashable type: 'ArrayObject'\n",
      "✓ Loaded pdf/SnT GPS_292호_최종.pdf: 69 pages\n",
      "✗ Error loading pdf/SnT GPS_283호_최종.pdf: unhashable type: 'ArrayObject'\n",
      "✓ Loaded pdf/SnT GPS_284호_최종.pdf: 80 pages\n",
      "✓ Loaded pdf/SnT GPS_291호_최종.pdf: 70 pages\n",
      "✓ Loaded pdf/SnT GPS_289호_최종.pdf: 67 pages\n",
      "Total documents loaded: 424\n",
      "Total splits: 752\n",
      "Total splits: 752\n",
      "▷ Vectorstore 생성 중...\n",
      "✓ Vectorstore 생성 완료.\n",
      "✓ Langsmith 프롬프트 로드 완료.\n",
      "\n",
      "✓ 모든 설정이 완료되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# import bs4\n",
    "# from langchain import hub\n",
    "# from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "# from langchain_community.document_loaders import WebBaseLoader\n",
    "# from langchain_community.vectorstores import Chroma\n",
    "# from langchain_core.output_parsers import StrOutputParser\n",
    "# from langchain_core.runnables import RunnablePassthrough\n",
    "# from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "# from langchain_huggingface import HuggingFacePipeline, HuggingFaceEmbeddings\n",
    "# from langchain_cohere import ChatCohere\n",
    "# from langchain_cohere import CohereEmbeddings\n",
    "# from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "# from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "# from langchain_upstage import ChatUpstage\n",
    "# from langchain_upstage import UpstageEmbeddings\n",
    "\n",
    "# import os\n",
    "# import getpass\n",
    "\n",
    "# #### INDEXING ####\n",
    "\n",
    "# if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "#     os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google AI API key: \")\n",
    "\n",
    "# if \"OPENAI_API_KEY\" not in os.environ:\n",
    "#     os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your Google AI API key: \")\n",
    "\n",
    "# from langchain_community.document_loaders import PyPDFLoader\n",
    "# import glob\n",
    "\n",
    "# embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\", chunk_size=100)\n",
    "\n",
    "# pdf_files = glob.glob(\"pdf/SnT GPS_*.pdf\")\n",
    "# print(f\"Found {len(pdf_files)} PDF files\")\n",
    "\n",
    "# all_docs = []\n",
    "# for pdf_file in pdf_files:\n",
    "#     try:\n",
    "#         loader = PyPDFLoader(pdf_file, extract_images=False)\n",
    "#         docs = loader.load()\n",
    "#         # 메타데이터 정리: unhashable 타입 처리\n",
    "#         for doc in docs:\n",
    "#             cleaned_metadata = {}\n",
    "#             for k, v in doc.metadata.items():\n",
    "#                 try:\n",
    "#                     if isinstance(v, (str, int, float, bool)):\n",
    "#                         cleaned_metadata[k] = v\n",
    "#                     else:\n",
    "#                         cleaned_metadata[k] = str(v)\n",
    "#                 except Exception:\n",
    "#                     cleaned_metadata[k] = \"unable_to_convert\"\n",
    "#             doc.metadata = cleaned_metadata\n",
    "#         all_docs.extend(docs)\n",
    "#         print(f\"✓ Loaded {pdf_file}: {len(docs)} pages\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"✗ Error loading {pdf_file}: {e}\")\n",
    "\n",
    "# print(f\"Total documents loaded: {len(all_docs)}\")\n",
    "\n",
    "# # Split\n",
    "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "# splits = text_splitter.split_documents(all_docs)\n",
    "# print(f\"Total splits: {len(splits)}\")\n",
    "\n",
    "# # Embed\n",
    "# vectorstore = Chroma.from_documents(\n",
    "#     documents=splits, \n",
    "#     embedding=embeddings,\n",
    "#     collection_name=\"rag_collection\"\n",
    "# )\n",
    "# retriever = vectorstore.as_retriever()\n",
    "\n",
    "# from langsmith import Client\n",
    "# client = Client(api_key=os.environ['LANGCHAIN_API_KEY'])\n",
    "# prompt = client.pull_prompt(\"rlm/rag-prompt\", include_model=True)\n",
    "\n",
    "# #### RETRIEVAL and GENERATION ####\n",
    "\n",
    "# def format_docs(docs):\n",
    "#     return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# # Split\n",
    "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "# splits = text_splitter.split_documents(all_docs)\n",
    "# print(f\"Total splits: {len(splits)}\")\n",
    "\n",
    "# # Embed\n",
    "# print(\"▷ Vectorstore 생성 중...\")\n",
    "# vectorstore = Chroma.from_documents(\n",
    "#     documents=splits, \n",
    "#     embedding=embeddings,\n",
    "#     collection_name=\"rag_collection\"\n",
    "# )\n",
    "# retriever = vectorstore.as_retriever()\n",
    "# print(\"✓ Vectorstore 생성 완료.\")\n",
    "\n",
    "# from langsmith import Client\n",
    "# client = Client(api_key=os.environ['LANGCHAIN_API_KEY'])\n",
    "# prompt = client.pull_prompt(\"rlm/rag-prompt\", include_model=True)\n",
    "# print(\"✓ Langsmith 프롬프트 로드 완료.\")\n",
    "\n",
    "\n",
    "# #### RETRIEVAL and GENERATION ####\n",
    "\n",
    "# def format_docs(docs):\n",
    "#     return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# print(\"\\n✓ 모든 설정이 완료되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7639ec64",
   "metadata": {},
   "source": [
    "### pypdfium2 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "64cbd3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pypdfium2\n",
      "Collecting pypdfium2\n",
      "  Downloading pypdfium2-5.0.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (67 kB)\n",
      "  Downloading pypdfium2-5.0.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (67 kB)\n",
      "Downloading pypdfium2-5.0.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0mDownloading pypdfium2-5.0.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pypdfium2\n",
      "Installing collected packages: pypdfium2\n",
      "Successfully installed pypdfium2-5.0.0\n",
      "Successfully installed pypdfium2-5.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pypdfium2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "136fb6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 unique PDF files\n",
      "✓ SnT GPS_283호_최종.pdf: 66 pages (sanitized)\n",
      "✓ SnT GPS_284호_최종.pdf: 80 pages (sanitized)\n",
      "✓ SnT GPS_285호_최종.pdf: 68 pages (sanitized)\n",
      "✓ SnT GPS_286호_최종.pdf: 68 pages (sanitized)\n",
      "✓ SnT GPS_287호_최종.pdf: 70 pages (sanitized)\n",
      "✓ SnT GPS_288호_최종.pdf: 67 pages (sanitized)\n",
      "✓ SnT GPS_289호_최종.pdf: 67 pages (sanitized)\n",
      "✓ SnT GPS_290호_최종본.pdf: 73 pages (sanitized)\n",
      "✓ SnT GPS_291호_최종.pdf: 70 pages (sanitized)\n",
      "✓ SnT GPS_292호_최종.pdf: 69 pages (sanitized)\n",
      "Load summary: success 10, failed 0\n",
      "Total documents loaded: 698\n",
      "Total splits: 1258\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Collection expecting embedding with dimension of 1536, got 3072",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 110\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal splits: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(splits)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# ===== Vector store =====\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m vectorstore \u001b[38;5;241m=\u001b[39m Chroma\u001b[38;5;241m.\u001b[39mfrom_documents(\n\u001b[1;32m    111\u001b[0m     documents\u001b[38;5;241m=\u001b[39msplits,\n\u001b[1;32m    112\u001b[0m     embedding\u001b[38;5;241m=\u001b[39membeddings,\n\u001b[1;32m    113\u001b[0m     collection_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrag_collection\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;66;03m# persist_directory=\"chroma_db\",\u001b[39;00m\n\u001b[1;32m    115\u001b[0m )\n\u001b[1;32m    116\u001b[0m retriever \u001b[38;5;241m=\u001b[39m vectorstore\u001b[38;5;241m.\u001b[39mas_retriever()\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat_docs\u001b[39m(docs: List[Document]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/langchain_community/vectorstores/chroma.py:887\u001b[0m, in \u001b[0;36mChroma.from_documents\u001b[0;34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m    885\u001b[0m texts \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m    886\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m--> 887\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_texts(\n\u001b[1;32m    888\u001b[0m     texts\u001b[38;5;241m=\u001b[39mtexts,\n\u001b[1;32m    889\u001b[0m     embedding\u001b[38;5;241m=\u001b[39membedding,\n\u001b[1;32m    890\u001b[0m     metadatas\u001b[38;5;241m=\u001b[39mmetadatas,\n\u001b[1;32m    891\u001b[0m     ids\u001b[38;5;241m=\u001b[39mids,\n\u001b[1;32m    892\u001b[0m     collection_name\u001b[38;5;241m=\u001b[39mcollection_name,\n\u001b[1;32m    893\u001b[0m     persist_directory\u001b[38;5;241m=\u001b[39mpersist_directory,\n\u001b[1;32m    894\u001b[0m     client_settings\u001b[38;5;241m=\u001b[39mclient_settings,\n\u001b[1;32m    895\u001b[0m     client\u001b[38;5;241m=\u001b[39mclient,\n\u001b[1;32m    896\u001b[0m     collection_metadata\u001b[38;5;241m=\u001b[39mcollection_metadata,\n\u001b[1;32m    897\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    898\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/langchain_community/vectorstores/chroma.py:843\u001b[0m, in \u001b[0;36mChroma.from_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m    835\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mchromadb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatch_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_batches\n\u001b[1;32m    837\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m create_batches(\n\u001b[1;32m    838\u001b[0m         api\u001b[38;5;241m=\u001b[39mchroma_collection\u001b[38;5;241m.\u001b[39m_client,\n\u001b[1;32m    839\u001b[0m         ids\u001b[38;5;241m=\u001b[39mids,\n\u001b[1;32m    840\u001b[0m         metadatas\u001b[38;5;241m=\u001b[39mmetadatas,\n\u001b[1;32m    841\u001b[0m         documents\u001b[38;5;241m=\u001b[39mtexts,\n\u001b[1;32m    842\u001b[0m     ):\n\u001b[0;32m--> 843\u001b[0m         chroma_collection\u001b[38;5;241m.\u001b[39madd_texts(\n\u001b[1;32m    844\u001b[0m             texts\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m batch[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m [],\n\u001b[1;32m    845\u001b[0m             metadatas\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m batch[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    846\u001b[0m             ids\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    847\u001b[0m         )\n\u001b[1;32m    848\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    849\u001b[0m     chroma_collection\u001b[38;5;241m.\u001b[39madd_texts(texts\u001b[38;5;241m=\u001b[39mtexts, metadatas\u001b[38;5;241m=\u001b[39mmetadatas, ids\u001b[38;5;241m=\u001b[39mids)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/langchain_community/vectorstores/chroma.py:299\u001b[0m, in \u001b[0;36mChroma.add_texts\u001b[0;34m(self, texts, metadatas, ids, **kwargs)\u001b[0m\n\u001b[1;32m    297\u001b[0m ids_with_metadata \u001b[38;5;241m=\u001b[39m [ids[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m non_empty_ids]\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_collection\u001b[38;5;241m.\u001b[39mupsert(\n\u001b[1;32m    300\u001b[0m         metadatas\u001b[38;5;241m=\u001b[39mmetadatas,\n\u001b[1;32m    301\u001b[0m         embeddings\u001b[38;5;241m=\u001b[39membeddings_with_metadatas,\n\u001b[1;32m    302\u001b[0m         documents\u001b[38;5;241m=\u001b[39mtexts_with_metadatas,\n\u001b[1;32m    303\u001b[0m         ids\u001b[38;5;241m=\u001b[39mids_with_metadata,\n\u001b[1;32m    304\u001b[0m     )\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected metadata value to be\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e):\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/chromadb/api/models/Collection.py:455\u001b[0m, in \u001b[0;36mCollection.upsert\u001b[0;34m(self, ids, embeddings, metadatas, documents, images, uris)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Update the embeddings, metadatas or documents for provided ids, or create them if they don't exist.\u001b[39;00m\n\u001b[1;32m    436\u001b[0m \n\u001b[1;32m    437\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;124;03m    None\u001b[39;00m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    446\u001b[0m upsert_request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_and_prepare_upsert_request(\n\u001b[1;32m    447\u001b[0m     ids\u001b[38;5;241m=\u001b[39mids,\n\u001b[1;32m    448\u001b[0m     embeddings\u001b[38;5;241m=\u001b[39membeddings,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    452\u001b[0m     uris\u001b[38;5;241m=\u001b[39muris,\n\u001b[1;32m    453\u001b[0m )\n\u001b[0;32m--> 455\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39m_upsert(\n\u001b[1;32m    456\u001b[0m     collection_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid,\n\u001b[1;32m    457\u001b[0m     ids\u001b[38;5;241m=\u001b[39mupsert_request[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mids\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    458\u001b[0m     embeddings\u001b[38;5;241m=\u001b[39mupsert_request[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    459\u001b[0m     metadatas\u001b[38;5;241m=\u001b[39mupsert_request[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadatas\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    460\u001b[0m     documents\u001b[38;5;241m=\u001b[39mupsert_request[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocuments\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    461\u001b[0m     uris\u001b[38;5;241m=\u001b[39mupsert_request[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muris\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    462\u001b[0m     tenant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtenant,\n\u001b[1;32m    463\u001b[0m     database\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatabase,\n\u001b[1;32m    464\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/chromadb/api/rust.py:493\u001b[0m, in \u001b[0;36mRustBindingsAPI._upsert\u001b[0;34m(self, collection_id, ids, embeddings, metadatas, documents, uris, tenant, database)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_upsert\u001b[39m(\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    491\u001b[0m     database: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m DEFAULT_DATABASE,\n\u001b[1;32m    492\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m--> 493\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbindings\u001b[38;5;241m.\u001b[39mupsert(\n\u001b[1;32m    494\u001b[0m         \u001b[38;5;28mstr\u001b[39m(collection_id),\n\u001b[1;32m    495\u001b[0m         ids,\n\u001b[1;32m    496\u001b[0m         embeddings,\n\u001b[1;32m    497\u001b[0m         metadatas,\n\u001b[1;32m    498\u001b[0m         documents,\n\u001b[1;32m    499\u001b[0m         uris,\n\u001b[1;32m    500\u001b[0m         tenant,\n\u001b[1;32m    501\u001b[0m         database,\n\u001b[1;32m    502\u001b[0m     )\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Collection expecting embedding with dimension of 1536, got 3072"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import getpass\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "# ===== Embeddings =====\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "try:\n",
    "    from langchain_community.document_loaders import PyPDFium2Loader\n",
    "except Exception:\n",
    "    PyPDFium2Loader = None\n",
    "\n",
    "# ===== LangChain core =====\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# ===== ENV =====\n",
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google AI API key: \")\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")\n",
    "\n",
    "# ===== JSON-safe 변환기 =====\n",
    "def to_json_safe(obj: Any) -> Any:\n",
    "    if obj is None or isinstance(obj, (str, int, float, bool)):\n",
    "        return obj\n",
    "    if isinstance(obj, (list, tuple, set)):\n",
    "        return [to_json_safe(x) for x in obj]\n",
    "    if isinstance(obj, dict):\n",
    "        return {str(k): to_json_safe(v) for k, v in obj.items()}\n",
    "    try:\n",
    "        import json\n",
    "        json.dumps(obj)\n",
    "        return obj\n",
    "    except Exception:\n",
    "        return str(obj)\n",
    "\n",
    "def rebuild_safe_documents(docs: List[Document], source_path: str) -> List[Document]:\n",
    "    safe: List[Document] = []\n",
    "    import os as _os\n",
    "    fname = _os.path.basename(source_path)\n",
    "    for d in docs:\n",
    "        page_num = d.metadata.get(\"page\", d.metadata.get(\"page_number\"))\n",
    "        safe_meta: Dict[str, Any] = {\n",
    "            \"source\": source_path,\n",
    "            \"file_name\": fname,\n",
    "            \"page\": int(page_num) if page_num is not None else None,\n",
    "        }\n",
    "        # 원본 메타데이터 추가 보존(충돌 시 safe_meta 우선)\n",
    "        extra = {k: v for k, v in d.metadata.items() if k not in (\"source\", \"file_name\", \"page\")}\n",
    "        safe_meta.update(to_json_safe(extra))\n",
    "        safe.append(Document(page_content=d.page_content, metadata=safe_meta))\n",
    "    return safe\n",
    "\n",
    "# ===== 로더 폴백 체인 =====\n",
    "def load_with_fallback(pdf_path: str) -> List[Document]:\n",
    "    errors = []\n",
    "\n",
    "    # 1) PyPDFLoader (가벼움, 그러나 pypdf 메타 이슈 발생 가능)\n",
    "    try:\n",
    "        docs = PyPDFLoader(pdf_path, extract_images=False).load()\n",
    "        return docs\n",
    "    except Exception as e:\n",
    "        errors.append((\"PyPDFLoader\", str(e)))\n",
    "\n",
    "    # 2) PyPDFium2Loader (텍스트 안정적, 메타잡음 적음)\n",
    "    if PyPDFium2Loader is not None:\n",
    "        try:\n",
    "            docs = PyPDFium2Loader(pdf_path).load()\n",
    "            return docs\n",
    "        except Exception as e:\n",
    "            errors.append((\"PyPDFium2Loader\", str(e)))\n",
    "\n",
    "\n",
    "    # 전부 실패\n",
    "    raise RuntimeError(\" all loaders failed: \" + \" | \".join([f\"{n}: {m}\" for n, m in errors]))\n",
    "\n",
    "# ===== 메인 =====\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\", chunk_size=100)\n",
    "\n",
    "# 중복 제거 및 정렬\n",
    "pdf_files = sorted(set(glob.glob(\"pdf/SnT GPS_*.pdf\")))\n",
    "print(f\"Found {len(pdf_files)} unique PDF files\")\n",
    "\n",
    "all_docs: List[Document] = []\n",
    "ok, fail = 0, 0\n",
    "\n",
    "for pdf_file in pdf_files:\n",
    "    try:\n",
    "        raw_docs = load_with_fallback(pdf_file)  # 여기서 로더 내부 오류를 흡수\n",
    "        safe_docs = rebuild_safe_documents(raw_docs, pdf_file)\n",
    "        all_docs.extend(safe_docs)\n",
    "        ok += 1\n",
    "        print(f\"✓ {os.path.basename(pdf_file)}: {len(safe_docs)} pages (sanitized)\")\n",
    "    except Exception as e:\n",
    "        fail += 1\n",
    "        print(f\"✗ {os.path.basename(pdf_file)}: {e}\")\n",
    "\n",
    "print(f\"Load summary: success {ok}, failed {fail}\")\n",
    "print(f\"Total documents loaded: {len(all_docs)}\")\n",
    "\n",
    "# ===== Split =====\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "splits = text_splitter.split_documents(all_docs)\n",
    "print(f\"Total splits: {len(splits)}\")\n",
    "\n",
    "# ===== Vector store =====\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embeddings,\n",
    "    collection_name=\"rag_collection\",\n",
    "    # persist_directory=\"chroma_db\",\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "def format_docs(docs: List[Document]) -> str:\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "print(\"Indexing complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d804eefd",
   "metadata": {},
   "source": [
    "## 3개 LLM 비교 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949e7d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Prompt: LangSmith에서 모델 없이 가져오기 =====\n",
    "from langsmith import Client\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "import os\n",
    "\n",
    "client = Client(api_key=os.environ.get(\"LANGCHAIN_API_KEY\"))\n",
    "# 중요: include_model=False 로 모델 분리\n",
    "prompt = client.pull_prompt(\"rlm/rag-prompt\", include_model=False)\n",
    "\n",
    "# retriever 검색 폭\n",
    "try:\n",
    "    retriever.search_kwargs = {\"k\": 6}\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ===== LLM 모델 설정 =====\n",
    "llm_models = {\n",
    "    \"GPT-3.5-turbo\": ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0, max_tokens=None),\n",
    "    \"GPT-4o-mini\": ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, max_tokens=None),\n",
    "    \"Gemini-2.0-flash\": ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-exp\", temperature=0, max_output_tokens=None)\n",
    "}\n",
    "\n",
    "# ===== RAG 체인 생성 =====\n",
    "rag_chains = {}\n",
    "for model_name, llm in llm_models.items():\n",
    "    chain = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    rag_chains[model_name] = chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aa1ad4ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AI 로보틱스(AI Robotics) 시장은 2023년 약 127억 달러에서 2030년에 얼마로 성장하나요?', '연방하원 선거 45일 후, 기독민주당(CDU)/기독사회당(CSU) 연합과 사민당(SPD)은 어떤 문서에 연정 합의했나요?', 'BCG는 각국 정부가 직면한 데이터 주권 및 보안 문제를 해결하고자 어떤 보고서를 발표했나요?']\n"
     ]
    }
   ],
   "source": [
    "# ===== 질문 리스트 =====\n",
    "questions = [\n",
    "    \"AI 로보틱스(AI Robotics) 시장은 2023년 약 127억 달러에서 2030년에 얼마로 성장하나요?\",\n",
    "    \"연방하원 선거 45일 후, 기독민주당(CDU)/기독사회당(CSU) 연합과 사민당(SPD)은 어떤 문서에 연정 합의했나요?\",\n",
    "    \"BCG는 각국 정부가 직면한 데이터 주권 및 보안 문제를 해결하고자 어떤 보고서를 발표했나요?\"\n",
    "]\n",
    "print(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9739a7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "Question 1: AI 로보틱스(AI Robotics) 시장은 2023년 약 127억 달러에서 2030년에 얼마로 성장하나요?\n",
      "====================================================================================================\n",
      "\n",
      "[검색된 관련 문서 정보]\n",
      "총 4개의 문서 청크가 검색되었습니다.\n",
      "  - 문서 1: pdf/SnT GPS_290호_최종본.pdf (페이지: 3)\n",
      "    내용 미리보기: 2\n",
      "피지컬 AI에 대한 시장 기대도 꾸준히 상승\n",
      "Grand View Research에 따르면, AI 로보틱스(AI Robotics) 시장은 2023년\n",
      "약 127억 달러에서 2030년 약 1,247억 달러(한화 약 168조 원)로 성장하며,\n",
      "연평균 38.5%의 높은 성장...\n",
      "  - 문서 2: pdf/SnT GPS_290호_최종본.pdf (페이지: 3)\n",
      "    내용 미리보기: 2\n",
      "피지컬 AI에 대한 시장 기대도 꾸준히 상승\n",
      "Grand View Research에 따르면, AI 로보틱스(AI Robotics) 시장은 2023년\n",
      "약 127억 달러에서 2030년 약 1,247억 달러(한화 약 168조 원)로 성장하며,\n",
      "연평균 38.5%의 높은 성장...\n",
      "  - 문서 3: pdf/SnT GPS_290호_최종본.pdf (페이지: 3)\n",
      "    내용 미리보기: 향후 관련 산업의 본격적인 성장과 확산이 가속화될 것으로 전망\n",
      "피지컬 AI는 의료, 제조, 건설, 물류, 국방 등 다양한 산업 전반의 지능화와\n",
      "자동화를 이끄는 핵심 기술로 부상하며, 국가 전략기술로서의 위상이 더욱\n",
      "강화될 것으로 예상\n",
      "이에 발맞춰 우리나라도 산업적 차원...\n",
      "  - 문서 4: pdf/SnT GPS_290호_최종본.pdf (페이지: 3)\n",
      "    내용 미리보기: 향후 관련 산업의 본격적인 성장과 확산이 가속화될 것으로 전망\n",
      "피지컬 AI는 의료, 제조, 건설, 물류, 국방 등 다양한 산업 전반의 지능화와\n",
      "자동화를 이끄는 핵심 기술로 부상하며, 국가 전략기술로서의 위상이 더욱\n",
      "강화될 것으로 예상\n",
      "이에 발맞춰 우리나라도 산업적 차원...\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "Question 2: 연방하원 선거 45일 후, 기독민주당(CDU)/기독사회당(CSU) 연합과 사민당(SPD)은 어떤 문서에 연정 합의했나요?\n",
      "====================================================================================================\n",
      "\n",
      "[검색된 관련 문서 정보]\n",
      "총 4개의 문서 청크가 검색되었습니다.\n",
      "  - 문서 1: pdf/SnT GPS_286호_최종.pdf (페이지: 7)\n",
      "    내용 미리보기: 6\n",
      "2신정부의 과학기술 및 혁신 정책 : 연정 협약(Koalitionsvertrag)3)\n",
      "연방하원 선거 45일 후, 기독민주당(CDU)/기독사회당(CSU) 연합과 사민당(SPD)은\n",
      "“독일에 대한 책임(Verantwortung für Deutschland)”이라는 문서에...\n",
      "  - 문서 2: pdf/SnT GPS_286호_최종.pdf (페이지: 7)\n",
      "    내용 미리보기: 6\n",
      "2신정부의 과학기술 및 혁신 정책 : 연정 협약(Koalitionsvertrag)3)\n",
      "연방하원 선거 45일 후, 기독민주당(CDU)/기독사회당(CSU) 연합과 사민당(SPD)은\n",
      "“독일에 대한 책임(Verantwortung für Deutschland)”이라는 문서에...\n",
      "  - 문서 3: pdf/SnT GPS_286호_최종.pdf (페이지: 7)\n",
      "    내용 미리보기: 6\n",
      "2신정부의 과학기술 및 혁신 정책 : 연정 협약(Koalitionsvertrag)3)\n",
      "연방하원 선거 45일 후, 기독민주당(CDU)/기독사회당(CSU) 연합과 사민당(SPD)은\n",
      "“독일에 대한 책임(Verantwortung für Deutschland)”이라는 문서에...\n",
      "  - 문서 4: pdf/SnT GPS_286호_최종.pdf (페이지: 7)\n",
      "    내용 미리보기: 6\n",
      "2신정부의 과학기술 및 혁신 정책 : 연정 협약(Koalitionsvertrag)3)\n",
      "연방하원 선거 45일 후, 기독민주당(CDU)/기독사회당(CSU) 연합과 사민당(SPD)은\n",
      "“독일에 대한 책임(Verantwortung für Deutschland)”이라는 문서에...\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "Question 3: BCG는 각국 정부가 직면한 데이터 주권 및 보안 문제를 해결하고자 어떤 보고서를 발표했나요?\n",
      "====================================================================================================\n",
      "\n",
      "[검색된 관련 문서 정보]\n",
      "총 4개의 문서 청크가 검색되었습니다.\n",
      "  - 문서 1: pdf/SnT GPS_291호_최종.pdf (페이지: 37)\n",
      "    내용 미리보기: 36\n",
      "8BCG, 소버린 클라우드 (Sovereign Cloud)  필요성 및 구축 전략 제시\n",
      "BCG는 각국 정부가 직면한 데이터 주권 및 보안 문제를 해결하고자 ‘소버린 클라우드  \n",
      "(주권 클라우드 )’의 필요성과 구축 전략을 설명하는 보고서* 발표(’25.6.)\n",
      "*S...\n",
      "  - 문서 2: pdf/SnT GPS_291호_최종.pdf (페이지: 37)\n",
      "    내용 미리보기: 36\n",
      "8BCG, 소버린 클라우드 (Sovereign Cloud)  필요성 및 구축 전략 제시\n",
      "BCG는 각국 정부가 직면한 데이터 주권 및 보안 문제를 해결하고자 ‘소버린 클라우드  \n",
      "(주권 클라우드 )’의 필요성과 구축 전략을 설명하는 보고서* 발표(’25.6.)\n",
      "*S...\n",
      "  - 문서 3: pdf/SnT GPS_291호_최종.pdf (페이지: 37)\n",
      "    내용 미리보기: 36\n",
      "8BCG, 소버린 클라우드 (Sovereign Cloud)  필요성 및 구축 전략 제시\n",
      "BCG는 각국 정부가 직면한 데이터 주권 및 보안 문제를 해결하고자 ‘소버린 클라우드  \n",
      "(주권 클라우드 )’의 필요성과 구축 전략을 설명하는 보고서* 발표(’25.6.)\n",
      "*S...\n",
      "  - 문서 4: pdf/SnT GPS_291호_최종.pdf (페이지: 37)\n",
      "    내용 미리보기: 36\n",
      "8BCG, 소버린 클라우드 (Sovereign Cloud)  필요성 및 구축 전략 제시\n",
      "BCG는 각국 정부가 직면한 데이터 주권 및 보안 문제를 해결하고자 ‘소버린 클라우드  \n",
      "(주권 클라우드 )’의 필요성과 구축 전략을 설명하는 보고서* 발표(’25.6.)\n",
      "*S...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 각 질문에 대해 관련 문서 검색\n",
    "for idx, question in enumerate(questions, 1):\n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(f\"Question {idx}: {question}\")\n",
    "    print(f\"{'='*100}\\n\")\n",
    "    \n",
    "    # 관련 문서 검색\n",
    "    relevant_docs = retriever.invoke(question)\n",
    "    print(f\"[검색된 관련 문서 정보]\")\n",
    "    print(f\"총 {len(relevant_docs)}개의 문서 청크가 검색되었습니다.\")\n",
    "    for i, doc in enumerate(relevant_docs, 1):\n",
    "        source = doc.metadata.get('source', 'Unknown')\n",
    "        page = doc.metadata.get('page', 'Unknown')\n",
    "        print(f\"  - 문서 {i}: {source} (페이지: {page})\")\n",
    "        print(f\"    내용 미리보기: {doc.page_content[:150]}...\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f7252607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AI 로보틱스(AI Robotics) 시장은 2023년 약 127억 달러에서 2030년에 얼마로 성장하나요?', '연방하원 선거 45일 후, 기독민주당(CDU)/기독사회당(CSU) 연합과 사민당(SPD)은 어떤 문서에 연정 합의했나요?', 'BCG는 각국 정부가 직면한 데이터 주권 및 보안 문제를 해결하고자 어떤 보고서를 발표했나요?']\n",
      "\n",
      "=== 질문: AI 로보틱스(AI Robotics) 시장은 2023년 약 127억 달러에서 2030년에 얼마로 성장하나요?\n",
      "\n",
      "[GPT-3.5-turbo]\n",
      "2023년 약 127억 달러에서 2030년 약 1,247억 달러로 성장할 것으로 전망됩니다. 연평균 38.5%의 높은 성장률을 기록할 것으로 예상됩니다. Grand View Research와 Statista의 예측에 따르면 AI 로보틱스 시장은 크게 성장할 것으로 전망됩니다.\n",
      "\n",
      "[GPT-4o-mini]\n",
      "AI 로보틱스(AI Robotics) 시장은 2023년 약 127억 달러에서 2030년 약 1,247억 달러로 성장할 것으로 전망됩니다. 이는 연평균 38.5%의 높은 성장률을 기록할 것으로 예상됩니다.\n",
      "\n",
      "[Gemini-2.0-flash]\n",
      "Grand View Research에 따르면, AI 로보틱스 시장은 2023년 약 127억 달러에서 2030년 약 1,247억 달러로 성장할 것으로 예상됩니다. Statista는 AI 로보틱스 시장이 2023년 약 127억 달러에서 2030년 약 643억 달러에 이를 것으로 전망합니다. 두 기관의 전망치에 차이가 있습니다.\n",
      "\n",
      "=== 질문: 연방하원 선거 45일 후, 기독민주당(CDU)/기독사회당(CSU) 연합과 사민당(SPD)은 어떤 문서에 연정 합의했나요?\n",
      "\n",
      "[GPT-3.5-turbo]\n",
      "독일에 대한 책임(Verantwortung für Deutschland)이라는 문서에 연정 합의했습니다.\n",
      "\n",
      "[GPT-4o-mini]\n",
      "연방하원 선거 45일 후, 기독민주당(CDU)/기독사회당(CSU) 연합과 사민당(SPD)은 \"독일에 대한 책임(Verantwortung für Deutschland)\"이라는 문서에 연정 합의했습니다. 이 문서는 독일의 연구개발, 기술혁신, 디지털 전환을 국가 경쟁력의 핵심으로 삼고, 2025년까지 과학기술 정책을 강화하려는 의지를 담고 있습니다.\n",
      "\n",
      "[Gemini-2.0-flash]\n",
      " 연방하원 선거 45일 후, 기독민주당(CDU)/기독사회당(CSU) 연합과 사민당(SPD)은 \"독일에 대한 책임(Verantwortung für Deutschland)\"이라는 문서에 연정 합의했습니다. 이 문서는 연정 협약이라고도 불립니다. 이 협약은 독일의 과학기술정책을 전방위로 강화하려는 의지를 담고 있습니다.\n",
      "\n",
      "=== 질문: BCG는 각국 정부가 직면한 데이터 주권 및 보안 문제를 해결하고자 어떤 보고서를 발표했나요?\n",
      "\n",
      "[GPT-3.5-turbo]\n",
      "BCG는 '소버린 클라우드 (주권 클라우드)'의 필요성과 구축 전략을 설명하는 보고서를 발표했습니다. 이 보고서는 각국 정부가 직면한 데이터 주권 및 보안 문제를 해결하고자 발표되었습니다. 해당 보고서는 'Sovereign Clouds Are Reshaping National Data Security'라는 주제로 발표되었습니다.\n",
      "\n",
      "[GPT-4o-mini]\n",
      "BCG는 각국 정부가 직면한 데이터 주권 및 보안 문제를 해결하기 위해 '소버린 클라우드'의 필요성과 구축 전략을 설명하는 보고서를 발표했습니다. 이 보고서는 2025년 6월에 발표되었으며, 국가의 민감한 데이터를 자국의 법적 관할권 내에서만 저장하고 운영할 수 있도록 설계된 네트워크 모델을 제안합니다. 소버린 클라우드는 기술적 및 운영적 자율성을 확보하고, 지정학적 갈등으로부터 데이터를 보호하는 데 중점을 두고 있습니다.\n",
      "\n",
      "[Gemini-2.0-flash]\n",
      "BCG는 각국 정부가 직면한 데이터 주권 및 보안 문제를 해결하고자 '소버린 클라우드(주권 클라우드)'의 필요성과 구축 전략을 설명하는 보고서를 발표했습니다. 해당 보고서는 Sovereign Clouds Are Reshaping National Data Security라는 제목으로, 데이터 주권 및 보안 문제 해결 방안을 제시합니다. 이 보고서는 2025년 6월에 발표되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 각 질문에 대해 3개 LLM으로 답변 생성\n",
    "for q in questions:\n",
    "    print(f\"\\n=== 질문: {q}\")\n",
    "    for name, chain in rag_chains.items():\n",
    "        try:\n",
    "            ans = chain.invoke(q)\n",
    "            print(f\"\\n[{name}]\\n{ans}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\n[{name}] 실패: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a1dc3712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Snowflake에 대해 설명해주세요.', '한국의 피지컬 AI 관련 정책 동향을 알려주세요.']\n"
     ]
    }
   ],
   "source": [
    "# ===== 질문 리스트 =====\n",
    "questions = [\n",
    "    \"Snowflake에 대해 설명해주세요.\",\n",
    "    \"한국의 피지컬 AI 관련 정책 동향을 알려주세요.\"\n",
    "]\n",
    "print(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a3303b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "Question 1: Snowflake에 대해 설명해주세요.\n",
      "====================================================================================================\n",
      "\n",
      "[검색된 관련 문서 정보]\n",
      "총 4개의 문서 청크가 검색되었습니다.\n",
      "  - 문서 1: pdf/SnT GPS_291호_최종.pdf (페이지: 13)\n",
      "    내용 미리보기: 12-데이터셋 라이브러리를 통해 단 한 줄의 코드만으로 데이터를 불러오고 \n",
      "전처리할 수 있는 편의성을 제공하며 , 연구자들의 데이터 활용 진입장벽을 \n",
      "획기적으로 감소\n",
      "-데이터셋 카드(Dataset Card) 표준을 통해 데이터 품질 문서화와 평가 도구를  \n",
      "제공함으로써...\n",
      "  - 문서 2: pdf/SnT GPS_291호_최종.pdf (페이지: 13)\n",
      "    내용 미리보기: 12-데이터셋 라이브러리를 통해 단 한 줄의 코드만으로 데이터를 불러오고 \n",
      "전처리할 수 있는 편의성을 제공하며 , 연구자들의 데이터 활용 진입장벽을 \n",
      "획기적으로 감소\n",
      "-데이터셋 카드(Dataset Card) 표준을 통해 데이터 품질 문서화와 평가 도구를  \n",
      "제공함으로써...\n",
      "  - 문서 3: pdf/SnT GPS_291호_최종.pdf (페이지: 13)\n",
      "    내용 미리보기: 12-데이터셋 라이브러리를 통해 단 한 줄의 코드만으로 데이터를 불러오고 \n",
      "전처리할 수 있는 편의성을 제공하며 , 연구자들의 데이터 활용 진입장벽을 \n",
      "획기적으로 감소\n",
      "-데이터셋 카드(Dataset Card) 표준을 통해 데이터 품질 문서화와 평가 도구를  \n",
      "제공함으로써...\n",
      "  - 문서 4: pdf/SnT GPS_291호_최종.pdf (페이지: 13)\n",
      "    내용 미리보기: 12-데이터셋 라이브러리를 통해 단 한 줄의 코드만으로 데이터를 불러오고 \n",
      "전처리할 수 있는 편의성을 제공하며 , 연구자들의 데이터 활용 진입장벽을 \n",
      "획기적으로 감소\n",
      "-데이터셋 카드(Dataset Card) 표준을 통해 데이터 품질 문서화와 평가 도구를  \n",
      "제공함으로써...\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "Question 2: 한국의 피지컬 AI 관련 정책 동향을 알려주세요.\n",
      "====================================================================================================\n",
      "\n",
      "[검색된 관련 문서 정보]\n",
      "총 4개의 문서 청크가 검색되었습니다.\n",
      "  - 문서 1: pdf/SnT GPS_290호_최종본.pdf (페이지: 15)\n",
      "    내용 미리보기: 14\n",
      "4 한국의 피지컬 AI 경쟁력 강화를 위한 시사점\n",
      "(국가 차원의 피지컬 AI 전략 수립) AI-로봇 융합 기술의 확산 흐름에 대응하기\n",
      "위해, 국가 차원의 피지컬 AI 전략 수립 필요\n",
      "현재 피지컬 AI(휴머노이드 로봇, 자율주행차, 드론, AGV & AMR 등)는 ...\n",
      "  - 문서 2: pdf/SnT GPS_290호_최종본.pdf (페이지: 15)\n",
      "    내용 미리보기: 14\n",
      "4 한국의 피지컬 AI 경쟁력 강화를 위한 시사점\n",
      "(국가 차원의 피지컬 AI 전략 수립) AI-로봇 융합 기술의 확산 흐름에 대응하기\n",
      "위해, 국가 차원의 피지컬 AI 전략 수립 필요\n",
      "현재 피지컬 AI(휴머노이드 로봇, 자율주행차, 드론, AGV & AMR 등)는 ...\n",
      "  - 문서 3: pdf/SnT GPS_290호_최종본.pdf (페이지: 16)\n",
      "    내용 미리보기: 고도화를 지원하는 모듈형 재교육 체계를 구축\n",
      "(신뢰할 수 있는 피지컬 AI 생태계 조성) 피지컬 AI의 안전성과 윤리성을 확보하고\n",
      "기술 수용성을 높이기 위해 신뢰할 수 있는 생태계를 조성\n",
      "안전･책임 이슈에 대응하기 위해 윤리･안전 기준을 반영한 인증제도 및 사전검증\n",
      "체...\n",
      "  - 문서 4: pdf/SnT GPS_290호_최종본.pdf (페이지: 16)\n",
      "    내용 미리보기: 고도화를 지원하는 모듈형 재교육 체계를 구축\n",
      "(신뢰할 수 있는 피지컬 AI 생태계 조성) 피지컬 AI의 안전성과 윤리성을 확보하고\n",
      "기술 수용성을 높이기 위해 신뢰할 수 있는 생태계를 조성\n",
      "안전･책임 이슈에 대응하기 위해 윤리･안전 기준을 반영한 인증제도 및 사전검증\n",
      "체...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 각 질문에 대해 관련 문서 검색\n",
    "for idx, question in enumerate(questions, 1):\n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(f\"Question {idx}: {question}\")\n",
    "    print(f\"{'='*100}\\n\")\n",
    "    \n",
    "    # 관련 문서 검색\n",
    "    relevant_docs = retriever.invoke(question)\n",
    "    print(f\"[검색된 관련 문서 정보]\")\n",
    "    print(f\"총 {len(relevant_docs)}개의 문서 청크가 검색되었습니다.\")\n",
    "    for i, doc in enumerate(relevant_docs, 1):\n",
    "        source = doc.metadata.get('source', 'Unknown')\n",
    "        page = doc.metadata.get('page', 'Unknown')\n",
    "        print(f\"  - 문서 {i}: {source} (페이지: {page})\")\n",
    "        print(f\"    내용 미리보기: {doc.page_content[:150]}...\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7a4e4842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 질문: Snowflake에 대해 설명해주세요.\n",
      "\n",
      "[GPT-3.5-turbo]\n",
      "Snowflake는 기업 간 안전한 데이터 교환과 AI용 데이터 마켓플레이스 역할을 수행하며, 외부 데이터를 쉽게 통합하여 활용할 수 있는 기능을 제공합니다. 또한, Snowflake는 대규모 합성 데이터셋을 출시하며 AI 학습용 데이터 유통 채널로 발전하고, 엔터프라이즈급 데이터 관리 체계를 구축하는데 도움을 줍니다. Snowflake는 데이터 거버넌스 체계를 완비하고 데이터 품질 관리를 동시에 달성하는데 도움이 됩니다.\n",
      "\n",
      "[GPT-4o-mini]\n",
      "Snowflake는 기업 간 안전한 데이터 교환과 AI용 데이터 마켓플레이스 역할을 수행하는 플랫폼입니다. 이 마켓플레이스에는 금융, 인구통계, 위치정보 등 다양한 외부 데이터가 등재되어 있어 기업들이 쉽게 통합하여 활용할 수 있습니다. 또한, ERP와 CRM 시스템의 데이터를 자동으로 연결하여 실시간 데이터 통합을 실현하고, 데이터 거버넌스 체계를 완비하여 규제 준수와 데이터 품질 관리를 동시에 달성합니다.\n",
      "\n",
      "[Gemini-2.0-flash]\n",
      "Snowflake는 기업 간 안전한 데이터 교환과 AI용 데이터 마켓플레이스 역할을 수행합니다. Snowflake 마켓플레이스에는 금융, 인구통계, 위치정보 등 수백 종의 외부 데이터가 등재되어 있어 기업들이 즉시 통합하여 활용할 수 있습니다. 또한, AI 학습용 데이터 유통 채널로 진화하고 엔터프라이즈급 데이터 관리 체계를 구축했습니다.\n",
      "\n",
      "=== 질문: 한국의 피지컬 AI 관련 정책 동향을 알려주세요.\n",
      "\n",
      "[GPT-3.5-turbo]\n",
      "한국은 피지컬 AI 분야에서 미국, 중국, EU, 일본 등과의 글로벌 경쟁에 대응하기 위해 대규모 투자를 통해 지능형 로봇 개발과 핵심 기술 자립을 촉진하고 있습니다. 또한 ICT 기술력과 제조업 기반을 활용하여 피지컬 AI 국가 전략을 수립하고 실용화 및 확산을 주도하고 있습니다. 미래에는 피지컬 AI가 다양한 산업 분야에서 노동력 부족 문제를 해결하고 생산성 향상, 비용 절감 등의 효과를 기대하고 있습니다.\n",
      "\n",
      "[GPT-4o-mini]\n",
      "한국은 피지컬 AI의 경쟁력 강화를 위해 국가 차원의 전략 수립이 필요하며, AI-로봇 융합 기술의 확산에 대응하고 있습니다. 제조, 물류, 교통, 국방 등 다양한 산업에서 노동력 부족 해결과 생산성 향상을 기대하고 있으며, 이를 위해 대규모 투자가 중요합니다. 또한, 통합 거버넌스를 구축하여 연구개발 투자와 규제 혁신을 체계적으로 추진할 필요가 있습니다.\n",
      "\n",
      "[Gemini-2.0-flash]\n",
      "한국은 피지컬 AI 경쟁력 강화를 위해 국가 차원의 전략 수립이 필요하며, ICT 기술력과 제조업 기반을 바탕으로 산업 및 공공 부문에서의 실용화와 확산을 주도할 필요가 있습니다. 또한, '피지컬 AI 전략위원회'와 같은 통합 거버넌스를 구축하여 연구개발 투자, 규제 혁신, 글로벌 협력 등을 체계적으로 총괄해야 합니다. 윤리 및 안전 기준을 반영한 인증제도와 사전 검증 체계를 도입하여 신뢰할 수 있는 피지컬 AI 생태계를 조성하는 것도 중요합니다.\n"
     ]
    }
   ],
   "source": [
    "# 각 질문에 대해 3개 LLM으로 답변 생성\n",
    "for q in questions:\n",
    "    print(f\"\\n=== 질문: {q}\")\n",
    "    for name, chain in rag_chains.items():\n",
    "        try:\n",
    "            ans = chain.invoke(q)\n",
    "            print(f\"\\n[{name}]\\n{ans}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\n[{name}] 실패: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab735ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
